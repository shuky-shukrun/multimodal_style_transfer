{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports \"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from utils import *\n",
    "from style_subnet import *\n",
    "from enhance_subnet import *\n",
    "from refine_subnet import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 1\n",
    "STYLE_NAME = \"picasso\"\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "CONTENT_WEIGHTS = [1, 1, 1]\n",
    "STYLE_WEIGHTS = [2e4, 1e5, 1e3] # Checkpoint single style\n",
    "#STYLE_WEIGHTS = [5e4, 8e4, 3e4] # Checkpoint two styles\n",
    "LAMBDAS = [1., 0.5, 0.25]\n",
    "REG = 1e-7\n",
    "LOG_INTERVAL = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Allow PIL to read truncated blocks when loading images \"\"\"\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add a seed to have reproducable results \"\"\"\n",
    "\n",
    "SEED = 1080\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configure training with or without cuda \"\"\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load coco dataset \"\"\"\n",
    "\n",
    "print(\"Loading dataset..\")\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "DATASET = scriptDir + \"/coco/\"\n",
    "transform = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n",
    "                                transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                transforms.ToTensor(), tensor_normalizer()])\n",
    "# http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\n",
    "train_dataset = datasets.ImageFolder(DATASET, transform)\n",
    "# http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Load Style Image \"\"\"\n",
    "\n",
    "style_img_256, style_img_512, style_img_1024 = style_loader(\n",
    "    \"styles/\" + STYLE_NAME + \".jpg\", device, [256, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(style_img_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Define Loss Network \"\"\"\n",
    "########## [\"relu1_1\", \"relu2_2\", \"relu3_3\", \"relu4_3\"] ##########\n",
    "StyleOutput = namedtuple(\"StyleOutput\", [\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\"])\n",
    "\n",
    "########## [\"relu2_2\"] ##########\n",
    "ContentOutput = namedtuple(\"ContentOutput\", [\"relu2_1\"])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.layer_name_mapping = {\n",
    "            '1': \"relu1_1\", '3': \"relu1_2\",\n",
    "            '6': \"relu2_1\", '8': \"relu2_2\",\n",
    "            '11': \"relu3_1\", '13': \"relu3_2\", '15': \"relu3_3\", '17': \"relu3_4\",\n",
    "            '20': \"relu4_1\", '22': \"relu4_2\", '24': \"relu4_3\", '26': \"relu4_4\",\n",
    "            '29': \"relu5_1\", '31': \"relu5_2\", '33': \"relu5_3\", '35': \"relu5_4\"\n",
    "        }\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        if mode == 'style':\n",
    "            ########## [\"relu1_1\", \"relu2_2\", \"relu3_3\", \"relu4_3\"] ##########\n",
    "            ########## ['1', '8', '15', '24'] ##########\n",
    "            layers = ['1', '6', '11', '20']\n",
    "        elif mode == 'content':\n",
    "            ########## [\"relu2_2\"] ##########\n",
    "            ########## ['8'] ##########\n",
    "            layers = ['6']\n",
    "        else:\n",
    "            print(\"Invalid mode. Select between 'style' and 'content'\")\n",
    "        output = {}\n",
    "        for name, module in self.vgg._modules.items():\n",
    "            x = module(x)\n",
    "            if name in layers:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        if mode == 'style':\n",
    "            return StyleOutput(**output)\n",
    "        else:\n",
    "            return ContentOutput(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Load and extract features from VGG16 \"\"\"\n",
    "\n",
    "print(\"Loading VGG..\")\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "loss_network = LossNetwork(vgg).to(device).eval()\n",
    "del vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Before training, compute the features of every resolution of the style image \"\"\"\n",
    "\n",
    "print(\"Computing style features..\")\n",
    "with torch.no_grad(): \n",
    "    style_loss_features_256 = loss_network(Variable(style_img_256), 'style')\n",
    "    style_loss_features_512 = loss_network(Variable(style_img_512), 'style')\n",
    "    style_loss_features_1024 = loss_network(Variable(style_img_1024), 'style')\n",
    "gram_style_256 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_256]\n",
    "gram_style_512 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_512]\n",
    "gram_style_1024 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Init Net and loss \"\"\"\n",
    "\n",
    "style_subnet = StyleSubnet().to(device)\n",
    "enhance_subnet = EnhanceSubnet().to(device)\n",
    "refine_subnet = RefineSubnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare Training \"\"\"\n",
    "\n",
    "max_iterations = min(10000, len(train_dataset))\n",
    "\n",
    "# init loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                             list(enhance_subnet.parameters()) +\n",
    "                             list(refine_subnet.parameters()), lr=LR)\n",
    "\n",
    "def getLosses(generated_img, resized_input_img, content_weight, style_weight, mse_loss, gram_style):\n",
    "    \n",
    "    # Compute features\n",
    "    generated_style_features = loss_network(generated_img, 'style')\n",
    "    generated_content_features = loss_network(generated_img, 'content')\n",
    "    target_content_features = loss_network(resized_input_img, 'content')\n",
    "    \n",
    "    # Content loss\n",
    "    target_content_features = Variable(target_content_features[0].data, requires_grad=False)\n",
    "    content_loss = content_weight * mse_loss(generated_content_features[0], target_content_features)\n",
    "    \n",
    "    # Style loss\n",
    "    style_loss = 0.\n",
    "    for m in range(len(generated_style_features)):\n",
    "        gram_s = gram_style[m]\n",
    "        gram_y = gram_matrix(generated_style_features[m])\n",
    "        style_loss += style_weight * mse_loss(gram_y, gram_s.expand_as(gram_y))\n",
    "    \n",
    "    # Regularization loss\n",
    "    reg_loss = REG * (\n",
    "        torch.sum(torch.abs(generated_img[:, :, :, :-1] - generated_img[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(generated_img[:, :, :-1, :] - generated_img[:, :, 1:, :])))\n",
    "    \n",
    "    return content_loss, style_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Perform Training \"\"\"\n",
    "\n",
    "style_subnet.train()\n",
    "enhance_subnet.train()\n",
    "refine_subnet.train()\n",
    "start = time.time()\n",
    "print(\"Start training on {}...\".format(device))\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "    log_counter = 0\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # update learning rate every 2000 iterations\n",
    "        if i % 2000 == 0 and i != 0:\n",
    "            LR = LR * 0.8\n",
    "            optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                                         list(enhance_subnet.parameters()) +\n",
    "                                         list(refine_subnet.parameters()), lr=LR)\n",
    "        \n",
    "        print(\"iteration: \" + str(i))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_in = x.clone()\n",
    "        \n",
    "        \"\"\" Style Subnet \"\"\"\n",
    "        x_in = Variable(x_in).to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_img_256, resized_input_img_256 = style_subnet(x_in)\n",
    "        resized_input_img_256 = Variable(resized_input_img_256.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        style_subnet_content_loss, style_subnet_style_loss, style_subnet_reg_loss = getLosses(\n",
    "            generated_img_256,\n",
    "            resized_input_img_256,\n",
    "            CONTENT_WEIGHTS[0],\n",
    "            STYLE_WEIGHTS[0],\n",
    "            mse_loss, gram_style_256)\n",
    "        \n",
    "        \"\"\" Enhance Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_256)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_512, resized_input_img_512 = enhance_subnet(x_in)\n",
    "        resized_input_img_512 = Variable(resized_input_img_512.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        enhance_subnet_content_loss, enhance_subnet_style_loss, enhance_subnet_reg_loss = getLosses(\n",
    "            generated_img_512,\n",
    "            resized_input_img_512,\n",
    "            CONTENT_WEIGHTS[1],\n",
    "            STYLE_WEIGHTS[1],\n",
    "            mse_loss, gram_style_512)\n",
    "        \n",
    "        \"\"\" Refine Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_512)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_1024, resized_input_img_1024 = refine_subnet(x_in)\n",
    "        resized_input_img_1024 = Variable(resized_input_img_1024.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        refine_subnet_content_loss, refine_subnet_style_loss, refine_subnet_reg_loss = getLosses(\n",
    "            generated_img_1024,\n",
    "            resized_input_img_1024,\n",
    "            CONTENT_WEIGHTS[2],\n",
    "            STYLE_WEIGHTS[2],\n",
    "            mse_loss, gram_style_1024)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = LAMBDAS[0] * (style_subnet_content_loss + style_subnet_style_loss + style_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[1] * (enhance_subnet_content_loss + enhance_subnet_style_loss + enhance_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[2] * (refine_subnet_content_loss + refine_subnet_style_loss + refine_subnet_reg_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Aggregated loss\n",
    "        agg_content_loss += style_subnet_content_loss.data + \\\n",
    "                            enhance_subnet_content_loss.data + \\\n",
    "                            refine_subnet_content_loss.data\n",
    "        agg_style_loss += style_subnet_style_loss.data + \\\n",
    "                          enhance_subnet_style_loss.data + \\\n",
    "                          refine_subnet_style_loss.data\n",
    "        \n",
    "        agg_reg_loss += style_subnet_reg_loss.data + \\\n",
    "                        enhance_subnet_reg_loss.data + \\\n",
    "                        refine_subnet_reg_loss.data\n",
    "        \n",
    "        \n",
    "        # log training process\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            log_counter += 1\n",
    "            hlp = log_counter * LOG_INTERVAL\n",
    "            time_per_pass = (time.time() - start) / hlp\n",
    "            estimated_time_left = (time_per_pass * (max_iterations - i))/3600\n",
    "            print(\"{} [{}/{}] time per pass: {:.2f}s  total time: {:.2f}s  estimated time left: {:.2f}h  content: {:.6f}  style: {:.6f}  reg: {:.6f}  total: {:.6f}\".format(\n",
    "                        time.ctime(), i+1, max_iterations,\n",
    "                        (time.time() - start) / hlp,\n",
    "                        time.time() - start,\n",
    "                        estimated_time_left,\n",
    "                        agg_content_loss / LOG_INTERVAL,\n",
    "                        agg_style_loss / LOG_INTERVAL,\n",
    "                        agg_reg_loss / LOG_INTERVAL,\n",
    "                        (agg_content_loss + agg_style_loss + agg_reg_loss) / LOG_INTERVAL))\n",
    "            agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "            imshow(x, title=\"input image\")\n",
    "            imshow(generated_img_256, title=\"generated_img_256\")\n",
    "            imshow(generated_img_512, title=\"generated_img_512\")\n",
    "            imshow(generated_img_1024, title=\"generated_img_1024\")\n",
    "\n",
    "        \"\"\"\n",
    "        if (i + 1) % (10 * LOG_INTERVAL) == 0:\n",
    "            save_image(generated_img_256, title=\"log_data/256_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            save_image(generated_img_512, title=\"log_data/512_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            save_image(generated_img_1024, title=\"log_data/1024_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            torch.save(style_subnet, 'log_data/trained_style_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            torch.save(enhance_subnet, 'log_data/trained_enhance_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            torch.save(refine_subnet, 'log_data/trained_refine_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            print(\"Images and models saved in /log_one_data\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Stop training after max iterations\n",
    "        if (i + 1) == max_iterations: break\n",
    "\n",
    "\"\"\" Save model \"\"\"\n",
    "torch.save(style_subnet, 'models/trained_style_subnet.pt')\n",
    "torch.save(enhance_subnet, 'models/trained_enhance_subnet.pt')\n",
    "torch.save(refine_subnet, 'models/trained_refine_subnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.1 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9d3bae0a0f66551680ef8a166f6b92cc2774d5d7901f027deb7bb883ed06d5ae"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}